{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Task 1: Data Importing & Transformation\n",
    "\n",
    "**Course:** CS424 - Visualization & Visual Analytics (Fall 2025)  \n",
    "**Dataset:** San Francisco Assessor Historical Secured Property Tax Rolls  \n",
    "**Source:** https://data.sfgov.org/Housing-and-Buildings/Assessor-Historical-Secured-Property-Tax-Rolls/wv5m-vpq2\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements Task 1 of Assignment 2, focusing on:\n",
    "1. Data importing and loading **from GeoJSON format**\n",
    "2. Data profiling and statistics computation\n",
    "3. Data transformation and cleaning\n",
    "4. Feature engineering and derived columns\n",
    "\n",
    "The dataset contains property tax assessment records for San Francisco from 2007-2023, with information about property characteristics, values, locations, and ownership.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "Before running this notebook, ensure you have the following libraries installed:\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy matplotlib seaborn\n",
    "pip install geopandas  # Required for GeoJSON loading\n",
    "pip install sodapy     # Optional, for SODA API access\n",
    "pip install pyarrow    # Required for Parquet export\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import geopandas for GeoJSON loading\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    print(\"✓ GeoPandas available\")\n",
    "except ImportError:\n",
    "    print(\"⚠ GeoPandas not installed. Install with: pip install geopandas\")\n",
    "    print(\"  GeoJSON loading will not work without geopandas.\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"\\nLibraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Importing\n",
    "\n",
    "### 2.1 Load the Dataset\n",
    "\n",
    "The dataset is available from San Francisco's Open Data Portal in multiple formats. \n",
    "We'll use **GeoJSON** format which includes geographic coordinates for spatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = r\"C:\\Users\\anand\\Desktop\\SEM 3\\CS 424\\Project\\Assessor_Historical_Secured_Property_Tax_Rolls_20251012.geojson\"\n",
    "\n",
    "gdf = gpd.read_file(local_file)\n",
    "df = pd.DataFrame(gdf.drop(columns='geometry'))\n",
    "df['latitude'] = gdf.geometry.y\n",
    "df['longitude'] = gdf.geometry.x\n",
    "\n",
    "print(f\"✓ Loaded {len(df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"=\"*80)\n",
    "print(\"DATA INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Display column information\n",
    "print(\"\\nColumn Information:\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values Analysis:\")\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values(\n",
    "    'Missing_Percentage', ascending=False\n",
    ")\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    display(missing_summary)\n",
    "else:\n",
    "    print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Profiling\n",
    "\n",
    "### 3.1 Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal distribution\n",
    "print(\"TEMPORAL ANALYSIS\")\n",
    "\n",
    "# Convert year to integer if needed\n",
    "df['year'] = pd.to_numeric(df['closed_roll_year'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Filter to relevant years (2015-2023)\n",
    "df_filtered = df[(df['year'] >= 2015) & (df['year'] <= 2023)].copy()\n",
    "\n",
    "print(f\"\\nOriginal dataset: {len(df):,} records\")\n",
    "print(f\"Filtered dataset (2015-2023): {len(df_filtered):,} records\")\n",
    "\n",
    "# Records per year\n",
    "year_counts = df_filtered['year'].value_counts().sort_index()\n",
    "print(\"\\nRecords per year:\")\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  {year}: {count:,} properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze geographic distribution\n",
    "print(\"GEOGRAPHIC ANALYSIS\")\n",
    "\n",
    "# Count unique neighborhoods\n",
    "n_neighborhoods = df_filtered['assessor_neighborhood'].nunique()\n",
    "print(f\"\\nTotal neighborhoods: {n_neighborhoods}\")\n",
    "\n",
    "# Top neighborhoods by property count\n",
    "top_neighborhoods = df_filtered['assessor_neighborhood'].value_counts().head(15)\n",
    "print(\"\\nTop 15 neighborhoods by property count:\")\n",
    "for idx, (neighborhood, count) in enumerate(top_neighborhoods.items(), 1):\n",
    "    print(f\"  {idx:2d}. {neighborhood}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Property Characteristics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze property characteristics\n",
    "print(\"PROPERTY CHARACTERISTICS\")\n",
    "\n",
    "# Convert numeric columns\n",
    "numeric_cols = ['number_of_bedrooms', 'number_of_bathrooms', 'number_of_rooms', \n",
    "                'number_of_stories', 'property_area', 'year_property_built']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_filtered.columns:\n",
    "        df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n",
    "\n",
    "# Bedrooms analysis\n",
    "print(\"\\nBedrooms:\")\n",
    "print(f\"  Mean: {df_filtered['number_of_bedrooms'].mean():.1f}\")\n",
    "print(f\"  Median: {df_filtered['number_of_bedrooms'].median():.0f}\")\n",
    "print(f\"  Range: {df_filtered['number_of_bedrooms'].min():.0f} - {df_filtered['number_of_bedrooms'].max():.0f}\")\n",
    "\n",
    "bedroom_dist = df_filtered['number_of_bedrooms'].value_counts().sort_index().head(10)\n",
    "print(\"\\n  Distribution (top 10):\")\n",
    "for bedrooms, count in bedroom_dist.items():\n",
    "    print(f\"    {bedrooms:.0f} bedrooms: {count:,} properties\")\n",
    "\n",
    "# Building age analysis\n",
    "print(\"\\nBuilding Age:\")\n",
    "print(f\"  Oldest: {df_filtered['year_property_built'].min():.0f}\")\n",
    "print(f\"  Newest: {df_filtered['year_property_built'].max():.0f}\")\n",
    "print(f\"  Median: {df_filtered['year_property_built'].median():.0f}\")\n",
    "print(f\"  Mean: {df_filtered['year_property_built'].mean():.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Financial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze property values\n",
    "print(\"FINANCIAL ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Convert financial columns to numeric\n",
    "value_cols = ['assessed_land_value', 'assessed_improvement_value']\n",
    "for col in value_cols:\n",
    "    if col in df_filtered.columns:\n",
    "        df_filtered[col] = pd.to_numeric(df_filtered[col], errors='coerce')\n",
    "\n",
    "# Create total assessed value\n",
    "df_filtered['total_assessed_value'] = (\n",
    "    df_filtered['assessed_land_value'] + df_filtered['assessed_improvement_value']\n",
    ")\n",
    "\n",
    "print(\"\\nTotal Assessed Value Statistics:\")\n",
    "print(f\"  Count: {df_filtered['total_assessed_value'].count():,}\")\n",
    "print(f\"  Mean: ${df_filtered['total_assessed_value'].mean():,.0f}\")\n",
    "print(f\"  Median: ${df_filtered['total_assessed_value'].median():,.0f}\")\n",
    "print(f\"  Std Dev: ${df_filtered['total_assessed_value'].std():,.0f}\")\n",
    "print(f\"  Min: ${df_filtered['total_assessed_value'].min():,.0f}\")\n",
    "print(f\"  Max: ${df_filtered['total_assessed_value'].max():,.0f}\")\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    value = df_filtered['total_assessed_value'].quantile(p/100)\n",
    "    print(f\"  {p}th: ${value:,.0f}\")\n",
    "\n",
    "# Land vs Improvement value comparison\n",
    "print(\"\\nLand vs Improvement Value:\")\n",
    "avg_land = df_filtered['assessed_land_value'].mean()\n",
    "avg_improvement = df_filtered['assessed_improvement_value'].mean()\n",
    "print(f\"  Average Land Value: ${avg_land:,.0f}\")\n",
    "print(f\"  Average Improvement Value: ${avg_improvement:,.0f}\")\n",
    "print(f\"  Land % of Total: {avg_land/(avg_land+avg_improvement)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Property Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze property types\n",
    "print(\"PROPERTY TYPE ANALYSIS\")\n",
    "\n",
    "# Property class distribution\n",
    "property_types = df_filtered['property_class_code_definition'].value_counts().head(10)\n",
    "print(\"\\nTop 10 Property Classes:\")\n",
    "for idx, (ptype, count) in enumerate(property_types.items(), 1):\n",
    "    print(f\"  {idx:2d}. {ptype}: {count:,}\")\n",
    "\n",
    "# Use code distribution\n",
    "use_types = df_filtered['use_definition'].value_counts().head(10)\n",
    "print(\"\\nTop 10 Use Definitions:\")\n",
    "for idx, (use, count) in enumerate(use_types.items(), 1):\n",
    "    print(f\"  {idx:2d}. {use}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Transformation\n",
    "\n",
    "### 4.1 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove invalid or outlier values\n",
    "print(\"DATA CLEANING\")\n",
    "\n",
    "# Store original size\n",
    "original_size = len(df_filtered)\n",
    "\n",
    "# Remove properties with zero or negative values\n",
    "df_clean = df_filtered[\n",
    "    (df_filtered['total_assessed_value'] > 0) &\n",
    "    (df_filtered['assessed_land_value'] > 0) &\n",
    "    (df_filtered['assessed_improvement_value'] >= 0)\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nRecords removed: {original_size - len(df_clean):,}\")\n",
    "print(f\"Cleaned dataset size: {len(df_clean):,}\")\n",
    "\n",
    "# Handle outliers (optional - using 99th percentile cap)\n",
    "value_99th = df_clean['total_assessed_value'].quantile(0.99)\n",
    "print(f\"\\n99th percentile value: ${value_99th:,.0f}\")\n",
    "print(f\"Properties above 99th percentile: {(df_clean['total_assessed_value'] > value_99th).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived columns\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "\n",
    "# 1. Land value percentage\n",
    "df_clean['land_value_pct'] = (\n",
    "    df_clean['assessed_land_value'] / df_clean['total_assessed_value'] * 100\n",
    ")\n",
    "print(\"\\n✓ Created: land_value_pct\")\n",
    "\n",
    "# 2. Building age\n",
    "current_year = 2023\n",
    "df_clean['building_age'] = current_year - df_clean['year_property_built']\n",
    "print(\"✓ Created: building_age\")\n",
    "\n",
    "# 3. Property density (value per sqft)\n",
    "df_clean['value_per_sqft'] = (\n",
    "    df_clean['total_assessed_value'] / df_clean['property_area']\n",
    ").replace([np.inf, -np.inf], np.nan)\n",
    "print(\"✓ Created: value_per_sqft\")\n",
    "\n",
    "# 4. Neighborhood label (simplified)\n",
    "df_clean['neighborhood'] = df_clean['assessor_neighborhood'].str.strip()\n",
    "print(\"✓ Created: neighborhood\")\n",
    "\n",
    "# 5. Property category (residential vs commercial)\n",
    "residential_keywords = ['Dwelling', 'Condominium', 'Apartment', 'Residential', 'Timeshare']\n",
    "df_clean['is_residential'] = df_clean['property_class_code_definition'].apply(\n",
    "    lambda x: any(kw in str(x) for kw in residential_keywords) if pd.notna(x) else False\n",
    ")\n",
    "print(\"✓ Created: is_residential\")\n",
    "\n",
    "# 6. Value tier\n",
    "def assign_value_tier(value):\n",
    "    if pd.isna(value):\n",
    "        return 'Unknown'\n",
    "    elif value < 300000:\n",
    "        return 'Budget'\n",
    "    elif value < 800000:\n",
    "        return 'Mid-range'\n",
    "    elif value < 2000000:\n",
    "        return 'High-end'\n",
    "    else:\n",
    "        return 'Luxury'\n",
    "\n",
    "df_clean['value_tier'] = df_clean['total_assessed_value'].apply(assign_value_tier)\n",
    "print(\"✓ Created: value_tier\")\n",
    "\n",
    "print(f\"\\nTotal new features created: 6\")\n",
    "print(f\"Final dataset shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Data Type Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize data types to reduce memory usage\n",
    "print(\"DATA TYPE OPTIMIZATION\")\n",
    "\n",
    "memory_before = df_clean.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nMemory before optimization: {memory_before:.1f} MB\")\n",
    "\n",
    "# Convert to appropriate types\n",
    "df_clean['year'] = df_clean['year'].astype('int16')\n",
    "df_clean['number_of_bedrooms'] = df_clean['number_of_bedrooms'].astype('int8')\n",
    "df_clean['number_of_bathrooms'] = df_clean['number_of_bathrooms'].astype('float16')\n",
    "df_clean['is_residential'] = df_clean['is_residential'].astype('bool')\n",
    "\n",
    "# Convert categorical columns\n",
    "categorical_cols = ['neighborhood', 'property_class_code_definition', \n",
    "                   'use_definition', 'value_tier']\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].astype('category')\n",
    "\n",
    "memory_after = df_clean.memory_usage(deep=True).sum() / 1024**2\n",
    "memory_reduction = (memory_before - memory_after) / memory_before * 100\n",
    "\n",
    "print(f\"Memory after optimization: {memory_after:.1f} MB\")\n",
    "print(f\"Memory reduction: {memory_reduction:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visual Data Summaries with Vega-Lite\n",
    "\n",
    "Create interactive visualizations using Vega-Lite (via Altair) to explore data distributions and relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Setup Altair (Python API for Vega-Lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# Configure Altair\n",
    "alt.data_transformers.enable('default', max_rows=None)  # Handle large datasets\n",
    "alt.renderers.enable('default')  # Use default renderer\n",
    "\n",
    "print(f\"✓ Altair version: {alt.__version__}\")\n",
    "print(f\"✓ Vega-Lite visualizations ready\")\n",
    "\n",
    "# For large datasets, we'll sample for interactive visualizations\n",
    "# Vega-Lite works best with <5000 rows for interactivity\n",
    "sample_size = 5000\n",
    "df_sample = df_clean.sample(n=min(sample_size, len(df_clean)), random_state=42)\n",
    "print(f\"\\nCreated sample of {len(df_sample):,} records for interactive visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Univariate Histograms with Binning\n",
    "\n",
    "Explore distributions of key numerical variables using interactive histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram 1: Total Assessed Value Distribution\n",
    "# Cap at 99th percentile for better visualization\n",
    "value_99th = df_clean['total_assessed_value'].quantile(0.99)\n",
    "df_viz = df_sample[df_sample['total_assessed_value'] <= value_99th].copy()\n",
    "\n",
    "chart_value_hist = alt.Chart(df_viz).mark_bar(\n",
    "    color='steelblue',\n",
    "    opacity=0.7\n",
    ").encode(\n",
    "    x=alt.X('total_assessed_value:Q',\n",
    "            bin=alt.Bin(maxbins=50),\n",
    "            title='Total Assessed Value ($)',\n",
    "            axis=alt.Axis(format='$,.0f')),\n",
    "    y=alt.Y('count()',\n",
    "            title='Number of Properties'),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('total_assessed_value:Q', bin=alt.Bin(maxbins=50), title='Value Range', format='$,.0f'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=300,\n",
    "    title='Distribution of Total Assessed Values (up to 99th percentile)'\n",
    ").interactive()\n",
    "\n",
    "chart_value_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram 2: Building Age Distribution\n",
    "chart_age_hist = alt.Chart(df_sample[df_sample['building_age'].notna()]).mark_bar(\n",
    "    color='purple',\n",
    "    opacity=0.7\n",
    ").encode(\n",
    "    x=alt.X('building_age:Q',\n",
    "            bin=alt.Bin(maxbins=40),\n",
    "            title='Building Age (years)',\n",
    "            scale=alt.Scale(domain=[0, 150])),\n",
    "    y=alt.Y('count()',\n",
    "            title='Number of Properties'),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('building_age:Q', bin=alt.Bin(maxbins=40), title='Age Range'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=300,\n",
    "    title='Distribution of Building Ages'\n",
    ").interactive()\n",
    "\n",
    "chart_age_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram 3: Land Value Percentage Distribution\n",
    "chart_land_pct_hist = alt.Chart(df_sample[df_sample['land_value_pct'].notna()]).mark_bar(\n",
    "    color='darkred',\n",
    "    opacity=0.7\n",
    ").encode(\n",
    "    x=alt.X('land_value_pct:Q',\n",
    "            bin=alt.Bin(maxbins=50),\n",
    "            title='Land Value as % of Total',\n",
    "            scale=alt.Scale(domain=[0, 100])),\n",
    "    y=alt.Y('count()',\n",
    "            title='Number of Properties'),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('land_value_pct:Q', bin=alt.Bin(maxbins=50), title='Land % Range', format='.1f'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=300,\n",
    "    title='Distribution of Land Value Percentage'\n",
    ").interactive()\n",
    "\n",
    "chart_land_pct_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Boxplots for Comparing Distributions\n",
    "\n",
    "Use boxplots to compare distributions across categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot 1: Property Values by Value Tier\n",
    "chart_value_boxplot = alt.Chart(df_sample).mark_boxplot(\n",
    "    size=40\n",
    ").encode(\n",
    "    x=alt.X('value_tier:N',\n",
    "            title='Value Tier',\n",
    "            sort=['Budget', 'Mid-range', 'High-end', 'Luxury']),\n",
    "    y=alt.Y('total_assessed_value:Q',\n",
    "            title='Total Assessed Value ($)',\n",
    "            scale=alt.Scale(type='log'),\n",
    "            axis=alt.Axis(format='$,.0f')),\n",
    "    color=alt.Color('value_tier:N',\n",
    "                    legend=None,\n",
    "                    scale=alt.Scale(scheme='category10')),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('value_tier:N', title='Tier'),\n",
    "        alt.Tooltip('total_assessed_value:Q', aggregate='median', title='Median Value', format='$,.0f'),\n",
    "        alt.Tooltip('total_assessed_value:Q', aggregate='q1', title='Q1', format='$,.0f'),\n",
    "        alt.Tooltip('total_assessed_value:Q', aggregate='q3', title='Q3', format='$,.0f')\n",
    "    ]\n",
    ").properties(\n",
    "    width=500,\n",
    "    height=350,\n",
    "    title='Property Value Distribution by Value Tier (log scale)'\n",
    ").interactive()\n",
    "\n",
    "chart_value_boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot 2: Building Age by Property Class (Top 5 types)\n",
    "top_property_types = df_clean['property_class_code_definition'].value_counts().head(5).index.tolist()\n",
    "df_top_types = df_sample[df_sample['property_class_code_definition'].isin(top_property_types)]\n",
    "\n",
    "chart_age_by_type = alt.Chart(df_top_types).mark_boxplot(\n",
    "    size=30\n",
    ").encode(\n",
    "    x=alt.X('property_class_code_definition:N',\n",
    "            title='Property Type',\n",
    "            axis=alt.Axis(labelAngle=-45)),\n",
    "    y=alt.Y('building_age:Q',\n",
    "            title='Building Age (years)',\n",
    "            scale=alt.Scale(domain=[0, 150])),\n",
    "    color=alt.Color('property_class_code_definition:N',\n",
    "                    legend=None),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('property_class_code_definition:N', title='Property Type'),\n",
    "        alt.Tooltip('building_age:Q', aggregate='median', title='Median Age'),\n",
    "        alt.Tooltip('count()', title='Sample Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=350,\n",
    "    title='Building Age Distribution by Property Type (Top 5)'\n",
    ").interactive()\n",
    "\n",
    "chart_age_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot 3: Land Value Percentage by Residential vs Non-Residential\n",
    "chart_land_by_residential = alt.Chart(df_sample).mark_boxplot(\n",
    "    size=60\n",
    ").encode(\n",
    "    x=alt.X('is_residential:N',\n",
    "            title='Property Type',\n",
    "            axis=alt.Axis(labelExpr=\"datum.value ? 'Residential' : 'Non-Residential'\")),\n",
    "    y=alt.Y('land_value_pct:Q',\n",
    "            title='Land Value as % of Total',\n",
    "            scale=alt.Scale(domain=[0, 100])),\n",
    "    color=alt.Color('is_residential:N',\n",
    "                    legend=alt.Legend(title='Type'),\n",
    "                    scale=alt.Scale(domain=[True, False], \n",
    "                                  range=['steelblue', 'orange'])),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('is_residential:N', title='Residential'),\n",
    "        alt.Tooltip('land_value_pct:Q', aggregate='median', title='Median Land %', format='.1f'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=350,\n",
    "    title='Land Value Percentage: Residential vs Non-Residential'\n",
    ").interactive()\n",
    "\n",
    "chart_land_by_residential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Faceted Small Multiples\n",
    "\n",
    "Create small multiples to compare distributions across multiple categories simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Multiples 1: Value Distribution by Year (2019-2023)\n",
    "df_recent = df_sample[df_sample['year'].between(2019, 2023)].copy()\n",
    "df_recent_capped = df_recent[df_recent['total_assessed_value'] <= value_99th]\n",
    "\n",
    "chart_value_by_year = alt.Chart(df_recent_capped).mark_bar(\n",
    "    opacity=0.7\n",
    ").encode(\n",
    "    x=alt.X('total_assessed_value:Q',\n",
    "            bin=alt.Bin(maxbins=30),\n",
    "            title='Total Assessed Value ($)',\n",
    "            axis=alt.Axis(format='$,.0s', labelAngle=-45)),\n",
    "    y=alt.Y('count()',\n",
    "            title='Count'),\n",
    "    color=alt.Color('year:N',\n",
    "                    legend=None),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('year:N', title='Year'),\n",
    "        alt.Tooltip('total_assessed_value:Q', bin=True, title='Value Range', format='$,.0f'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=180,\n",
    "    height=150\n",
    ").facet(\n",
    "    facet=alt.Facet('year:N', title='Year'),\n",
    "    columns=3\n",
    ").properties(\n",
    "    title='Property Value Distribution by Year (2019-2023)'\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "chart_value_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Multiples 2: Building Age by Top 6 Neighborhoods\n",
    "top_6_neighborhoods = df_clean['neighborhood'].value_counts().head(6).index.tolist()\n",
    "df_top_nbhd = df_sample[df_sample['neighborhood'].isin(top_6_neighborhoods)]\n",
    "\n",
    "chart_age_by_neighborhood = alt.Chart(df_top_nbhd).mark_bar(\n",
    "    opacity=0.7,\n",
    "    color='purple'\n",
    ").encode(\n",
    "    x=alt.X('building_age:Q',\n",
    "            bin=alt.Bin(maxbins=20),\n",
    "            title='Building Age',\n",
    "            scale=alt.Scale(domain=[0, 150])),\n",
    "    y=alt.Y('count()',\n",
    "            title='Count'),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('neighborhood:N', title='Neighborhood'),\n",
    "        alt.Tooltip('building_age:Q', bin=True, title='Age Range'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=180,\n",
    "    height=150\n",
    ").facet(\n",
    "    facet=alt.Facet('neighborhood:N', title=None),\n",
    "    columns=3\n",
    ").properties(\n",
    "    title='Building Age Distribution by Top 6 Neighborhoods'\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "chart_age_by_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small Multiples 3: Bedrooms Distribution by Value Tier\n",
    "df_bedrooms = df_sample[df_sample['number_of_bedrooms'] <= 5].copy()\n",
    "\n",
    "chart_bedrooms_by_tier = alt.Chart(df_bedrooms).mark_bar(\n",
    "    opacity=0.7\n",
    ").encode(\n",
    "    x=alt.X('number_of_bedrooms:O',\n",
    "            title='Number of Bedrooms'),\n",
    "    y=alt.Y('count()',\n",
    "            title='Count'),\n",
    "    color=alt.Color('value_tier:N',\n",
    "                    legend=None),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('value_tier:N', title='Value Tier'),\n",
    "        alt.Tooltip('number_of_bedrooms:O', title='Bedrooms'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=150,\n",
    "    height=150\n",
    ").facet(\n",
    "    facet=alt.Facet('value_tier:N', \n",
    "                    title='Value Tier',\n",
    "                    sort=['Budget', 'Mid-range', 'High-end', 'Luxury']),\n",
    "    columns=4\n",
    ").properties(\n",
    "    title='Bedroom Distribution by Value Tier'\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")\n",
    "\n",
    "chart_bedrooms_by_tier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Interactive Selection and Filtering\n",
    "\n",
    "Create linked visualizations with interactive selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful when you want to focus on a subset and see statistical summaries\n",
    "\n",
    "# Create separate brush for filtered version\n",
    "brush_filter = alt.selection_interval(encodings=['x'])\n",
    "\n",
    "# Same histogram\n",
    "hist_filter = alt.Chart(df_sample).mark_bar(\n",
    "    opacity=0.7\n",
    ").encode(\n",
    "    x=alt.X('building_age:Q',\n",
    "            bin=alt.Bin(maxbins=30),\n",
    "            title='Building Age (years)'),\n",
    "    y=alt.Y('count()',\n",
    "            title='Count'),\n",
    "    color=alt.condition(brush_filter, \n",
    "                       alt.value('steelblue'), \n",
    "                       alt.value('lightgray')),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('building_age:Q', bin=True, title='Age Range'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=200,\n",
    "    title='Select Age Range (brush over bars)'\n",
    ").add_params(\n",
    "    brush_filter\n",
    ")\n",
    "\n",
    "# Scatterplot with filtering + count\n",
    "scatter_filtered = alt.Chart(df_sample).mark_circle(\n",
    "    size=40,\n",
    "    opacity=0.6\n",
    ").encode(\n",
    "    x=alt.X('property_area:Q',\n",
    "            title='Property Area (sqft)',\n",
    "            scale=alt.Scale(type='log')),\n",
    "    y=alt.Y('total_assessed_value:Q',\n",
    "            title='Total Assessed Value ($)',\n",
    "            scale=alt.Scale(type='log'),\n",
    "            axis=alt.Axis(format='$,.0s')),\n",
    "    color=alt.Color('building_age:Q', \n",
    "                   scale=alt.Scale(scheme='viridis'),\n",
    "                   legend=alt.Legend(title='Building Age')),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('building_age:Q', title='Building Age'),\n",
    "        alt.Tooltip('property_area:Q', title='Property Area (sqft)', format=',.0f'),\n",
    "        alt.Tooltip('total_assessed_value:Q', title='Value', format='$,.0f'),\n",
    "        alt.Tooltip('neighborhood:N', title='Neighborhood')\n",
    "    ]\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=250\n",
    ").transform_filter(\n",
    "    brush_filter\n",
    ").interactive()\n",
    "\n",
    "# Add a text annotation showing count of filtered points\n",
    "text_count = alt.Chart(df_sample).mark_text(\n",
    "    align='left',\n",
    "    baseline='top',\n",
    "    fontSize=12,\n",
    "    dx=5,\n",
    "    dy=5\n",
    ").encode(\n",
    "    text=alt.Text('count():Q', format=',.0f')\n",
    ").transform_filter(\n",
    "    brush_filter\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=30,\n",
    "    title='Selected Properties Count'\n",
    ")\n",
    "\n",
    "# Combine all three\n",
    "chart_filtered = alt.vconcat(\n",
    "    hist_filter,\n",
    "    text_count,\n",
    "    scatter_filtered\n",
    ").properties(\n",
    "    title='Interactive Linked Views: Brush Selection (Filter Mode)'\n",
    ")\n",
    "\n",
    "chart_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Selection 2: Click selection on neighborhoods\n",
    "\n",
    "# Top 10 neighborhoods for clarity\n",
    "top_10_neighborhoods = df_clean['neighborhood'].value_counts().head(10).index.tolist()\n",
    "df_top10 = df_sample[df_sample['neighborhood'].isin(top_10_neighborhoods)]\n",
    "\n",
    "# Create click selection\n",
    "click = alt.selection_point(fields=['neighborhood'], empty=False)\n",
    "\n",
    "# Bar chart of neighborhoods with click selection\n",
    "nbhd_bars = alt.Chart(df_top10).mark_bar().encode(\n",
    "    y=alt.Y('neighborhood:N',\n",
    "            title='Neighborhood',\n",
    "            sort='-x'),\n",
    "    x=alt.X('count()',\n",
    "            title='Number of Properties (in sample)'),\n",
    "    color=alt.condition(click,\n",
    "                       alt.Color('neighborhood:N', legend=None),\n",
    "                       alt.value('lightgray')),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('neighborhood:N', title='Neighborhood'),\n",
    "        alt.Tooltip('count()', title='Count in Sample')\n",
    "    ]\n",
    ").properties(\n",
    "    width=300,\n",
    "    height=300,\n",
    "    title='Click to Select Neighborhood'\n",
    ").add_params(\n",
    "    click\n",
    ")\n",
    "\n",
    "# Histogram filtered by neighborhood selection\n",
    "value_hist_filtered = alt.Chart(df_top10).mark_bar(\n",
    "    opacity=0.7\n",
    ").encode(\n",
    "    x=alt.X('total_assessed_value:Q',\n",
    "            bin=alt.Bin(maxbins=30),\n",
    "            title='Total Assessed Value ($)',\n",
    "            axis=alt.Axis(format='$,.0s')),\n",
    "    y=alt.Y('count()',\n",
    "            title='Count'),\n",
    "    color=alt.Color('neighborhood:N', legend=None),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('neighborhood:N', title='Neighborhood'),\n",
    "        alt.Tooltip('total_assessed_value:Q', bin=True, title='Value Range', format='$,.0f'),\n",
    "        alt.Tooltip('count()', title='Count')\n",
    "    ]\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300,\n",
    "    title='Value Distribution for Selected Neighborhood'\n",
    ").transform_filter(\n",
    "    click\n",
    ").interactive()\n",
    "\n",
    "# Combine horizontally\n",
    "chart_click_selection = alt.hconcat(nbhd_bars, value_hist_filtered).properties(\n",
    "    title='Interactive Click Selection: Neighborhood Filter'\n",
    ")\n",
    "\n",
    "chart_click_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Vega-Lite Summary\n",
    "\n",
    "Summary of interactive visualizations created with Vega-Lite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "viz_summary = {\n",
    "    'Univariate Histograms': 3,\n",
    "    'Boxplots': 3,\n",
    "    'Faceted Small Multiples': 3,\n",
    "    'Interactive Selections': 2\n",
    "}\n",
    "\n",
    "print(\"\\nVisualizations Created:\")\n",
    "total = 0\n",
    "for viz_type, count in viz_summary.items():\n",
    "    print(f\"  • {viz_type}: {count}\")\n",
    "    total += count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final dataset summary\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_clean.shape}\")\n",
    "print(f\"Memory usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"Date range: {df_clean['year'].min()} - {df_clean['year'].max()}\")\n",
    "print(f\"Number of neighborhoods: {df_clean['neighborhood'].nunique()}\")\n",
    "\n",
    "print(\"\\nColumn List:\")\n",
    "for i, col in enumerate(df_clean.columns, 1):\n",
    "    dtype = df_clean[col].dtype\n",
    "    print(f\"  {i:2d}. {col:40s} ({dtype})\")\n",
    "\n",
    "print(\"\\nKey Statistics:\")\n",
    "print(f\"  Total properties: {len(df_clean):,}\")\n",
    "print(f\"  Median value: ${df_clean['total_assessed_value'].median():,.0f}\")\n",
    "print(f\"  Residential properties: {df_clean['is_residential'].sum():,} ({df_clean['is_residential'].sum()/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(\"\\nSample of cleaned and transformed dataset:\")\n",
    "display(df_clean.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Dataset\n",
    "\n",
    "We'll export the cleaned dataset in multiple formats:\n",
    "- **CSV**: Universal compatibility, human-readable\n",
    "- **Parquet**: Efficient columnar storage, faster I/O\n",
    "- **GeoJSON**: Geographic data for mapping applications\n",
    "- **Pickle**: Python-specific, fastest loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Verify Geographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we have geographic coordinates (already extracted from GeoJSON)\n",
    "print(\"Verifying geographic coordinates...\")\n",
    "\n",
    "if 'latitude' in df_clean.columns and 'longitude' in df_clean.columns:\n",
    "    valid_coords = df_clean[['latitude', 'longitude']].notna().all(axis=1).sum()\n",
    "    print(f\"✓ Coordinates available for {valid_coords:,} properties ({valid_coords/len(df_clean)*100:.1f}%)\")\n",
    "    \n",
    "    # Show coordinate ranges (SF boundaries)\n",
    "    if valid_coords > 0:\n",
    "        lat_valid = df_clean['latitude'].dropna()\n",
    "        lon_valid = df_clean['longitude'].dropna()\n",
    "        print(f\"\\n  Latitude range: {lat_valid.min():.4f} to {lat_valid.max():.4f}\")\n",
    "        print(f\"  Longitude range: {lon_valid.min():.4f} to {lon_valid.max():.4f}\")\n",
    "        print(f\"  Expected SF bounds: Lat 37.7-37.8, Lon -122.5 to -122.4\")\n",
    "else:\n",
    "    print(\"Warning: No coordinate columns found.\")\n",
    "    print(\"Creating placeholder coordinates for export...\")\n",
    "    df_clean['latitude'] = None\n",
    "    df_clean['longitude'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save as CSV\n",
    "csv_file = 'sf_property_data_clean.csv'\n",
    "print(\"Saving to CSV format...\")\n",
    "df_clean.to_csv(csv_file, index=False)\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    file_size = os.path.getsize(csv_file) / 1024**2\n",
    "    print(f\"✓ CSV saved: {csv_file}\")\n",
    "    print(f\"  File size: {file_size:.1f} MB\")\n",
    "    print(f\"  Rows: {len(df_clean):,}\")\n",
    "    print(f\"  Columns: {len(df_clean.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Export to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as Parquet (efficient columnar format)\n",
    "parquet_file = 'sf_property_data_clean.parquet'\n",
    "print(\"\\nSaving to Parquet format...\")\n",
    "\n",
    "# Convert category types back to string for Parquet compatibility\n",
    "df_parquet = df_clean.copy()\n",
    "for col in df_parquet.select_dtypes(include=['category']).columns:\n",
    "    df_parquet[col] = df_parquet[col].astype(str)\n",
    "\n",
    "df_parquet.to_parquet(parquet_file, engine='pyarrow', compression='snappy', index=False)\n",
    "\n",
    "if os.path.exists(parquet_file):\n",
    "    file_size = os.path.getsize(parquet_file) / 1024**2\n",
    "    print(f\"✓ Parquet saved: {parquet_file}\")\n",
    "    print(f\"  File size: {file_size:.1f} MB\")\n",
    "    print(f\"  Compression: snappy\")\n",
    "    print(f\"  Size reduction vs CSV: {(1 - file_size/(os.path.getsize(csv_file)/1024**2))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Export to GeoJSON\n",
    "\n",
    "We'll export the geographic data back to GeoJSON format for mapping applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoJSON for properties with valid coordinates\n",
    "geojson_file = 'sf_property_data_geo.geojson'\n",
    "print(\"Saving to GeoJSON format...\")\n",
    "\n",
    "# Filter to properties with valid coordinates\n",
    "df_geo = df_clean[df_clean[['latitude', 'longitude']].notna().all(axis=1)].copy()\n",
    "\n",
    "if len(df_geo) > 0:\n",
    "    print(f\"  Creating GeoDataFrame from {len(df_geo):,} properties with coordinates...\")\n",
    "    \n",
    "    # Limit to reasonable sample size for GeoJSON (to keep file manageable)\n",
    "    sample_size = min(50000, len(df_geo))\n",
    "    df_geo_sample = df_geo.sample(n=sample_size, random_state=42) if len(df_geo) > sample_size else df_geo\n",
    "    \n",
    "    # Create Point geometries from coordinates\n",
    "    from shapely.geometry import Point\n",
    "    \n",
    "    geometry = [Point(xy) for xy in zip(df_geo_sample['longitude'], df_geo_sample['latitude'])]\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf_export = gpd.GeoDataFrame(df_geo_sample, geometry=geometry, crs=\"EPSG:4326\")\n",
    "    \n",
    "    # Select key columns for export (avoid overly large file)\n",
    "    export_columns = [\n",
    "        'year', 'neighborhood', 'parcel_number',\n",
    "        'total_assessed_value', 'assessed_land_value', 'assessed_improvement_value',\n",
    "        'property_class_code_definition', 'number_of_bedrooms', 'number_of_bathrooms',\n",
    "        'property_area', 'building_age', 'land_value_pct',\n",
    "        'year_property_built', 'value_tier', 'is_residential',\n",
    "        'geometry'\n",
    "    ]\n",
    "    \n",
    "    # Filter to available columns\n",
    "    available_export_cols = [col for col in export_columns if col in gdf_export.columns or col == 'geometry']\n",
    "    gdf_export_subset = gdf_export[available_export_cols]\n",
    "    gdf_export_subset = gdf_export_subset.astype({\n",
    "        col: \"float64\" for col in gdf_export_subset.select_dtypes(include=[\"float16\", \"float32\"]).columns\n",
    "    })\n",
    "    # Export to GeoJSON\n",
    "    gdf_export_subset.to_file(geojson_file, driver='GeoJSON')\n",
    "    \n",
    "    if os.path.exists(geojson_file):\n",
    "        file_size = os.path.getsize(geojson_file) / 1024**2\n",
    "        print(f\"✓ GeoJSON saved: {geojson_file}\")\n",
    "        print(f\"  File size: {file_size:.1f} MB\")\n",
    "        print(f\"  Features: {len(gdf_export_subset):,}\")\n",
    "        print(f\"  Properties per feature: {len(available_export_cols) - 1}\")  # -1 for geometry\n",
    "        print(f\"  CRS: EPSG:4326 (WGS84)\")\n",
    "        if len(df_geo) > sample_size:\n",
    "            print(f\"  Note: Sampled {sample_size:,} of {len(df_geo):,} properties with coordinates\")\n",
    "            print(f\"        For full export, adjust sample_size in the code\")\n",
    "else:\n",
    "    print(\"  Warning: No properties with valid coordinates found. GeoJSON not created.\")\n",
    "    print(\"  This may occur if the data source didn't include geographic coordinates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Save Pickle for Fast Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as pickle for fastest loading in Python\n",
    "pickle_file = 'sf_property_data_clean.pkl'\n",
    "print(\"\\nSaving to Pickle format...\")\n",
    "df_clean.to_pickle(pickle_file)\n",
    "\n",
    "if os.path.exists(pickle_file):\n",
    "    file_size = os.path.getsize(pickle_file) / 1024**2\n",
    "    print(f\"✓ Pickle saved: {pickle_file}\")\n",
    "    print(f\"  File size: {file_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Export Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of all exports\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "export_files = [\n",
    "    ('CSV', csv_file, 'Universal compatibility, human-readable'),\n",
    "    ('Parquet', parquet_file, 'Efficient columnar storage, ~10x faster than CSV'),\n",
    "    ('GeoJSON', geojson_file, 'Geographic data for mapping (QGIS, Leaflet, etc.)'),\n",
    "    ('Pickle', pickle_file, 'Python-specific, fastest loading')\n",
    "]\n",
    "\n",
    "for format_name, filename, description in export_files:\n",
    "    if os.path.exists(filename):\n",
    "        size_mb = os.path.getsize(filename) / 1024**2\n",
    "        print(f\"\\n{format_name.upper()}:\")\n",
    "        print(f\"  File: {filename}\")\n",
    "        print(f\"  Size: {size_mb:.1f} MB\")\n",
    "        print(f\"  Use: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
